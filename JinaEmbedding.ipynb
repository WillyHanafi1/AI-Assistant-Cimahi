{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1884d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Environment variables loaded successfully!\")\n",
    "print(f\"JINA_API_KEY loaded: {'✅' if os.getenv('JINA_API_KEY') else '❌'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6200cfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "import json\n",
    "\n",
    "def extract_text_from_pdfs(input_folder, output_file):\n",
    "    \"\"\"Extract text from PDF files in the specified folder and save to a JSON file.\"\"\"\n",
    "    extracted_data = []\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith('.pdf'):\n",
    "            filepath = os.path.join(input_folder, filename)\n",
    "            with pdfplumber.open(filepath) as pdf:\n",
    "                text = ''.join([page.extract_text() for page in pdf.pages])\n",
    "                extracted_data.append({\"filename\": filename, \"text\": text})\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(extracted_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Example usage\n",
    "extract_text_from_pdfs('./docs', 'extracted.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b9dd3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed Perda Kota Cimahi No. 8 Tahun 2014.pdf part 1/3\n",
      "Successfully processed Perda Kota Cimahi No. 8 Tahun 2014.pdf part 2/3\n",
      "Successfully processed Perda Kota Cimahi No. 8 Tahun 2014.pdf part 3/3\n",
      "Successfully processed Perda No 8 2011 IMB.pdf part 1/2\n",
      "Successfully processed Perda No 8 2011 IMB.pdf part 2/2\n",
      "Successfully processed Perwal Cimahi 6 2024.pdf part 1/3\n",
      "Successfully processed Perwal Cimahi 6 2024.pdf part 2/3\n",
      "Successfully processed Perwal Cimahi 6 2024.pdf part 3/3\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "def split_text_by_length(text, max_length):\n",
    "    \"\"\"Split text into chunks with a maximum length.\"\"\"\n",
    "    return [text[i:i+max_length] for i in range(0, len(text), max_length)]\n",
    "\n",
    "def chunk_documents(input_file, output_file, api_url, api_key):\n",
    "    \"\"\"Chunk documents using the Jina Segment API and save the results to a JSON file.\"\"\"\n",
    "    # Load documents from input file\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        documents = json.load(f)\n",
    "\n",
    "    chunked_data = []\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {api_key}'\n",
    "    }\n",
    "    max_api_length = 64000  # 64k chars, a bit less for safety\n",
    "\n",
    "    for doc in documents:\n",
    "        if not doc.get(\"text\"):\n",
    "            print(f\"Skipping document {doc.get('filename', 'unknown')} due to missing or empty text.\")\n",
    "            continue\n",
    "\n",
    "        text_chunks = split_text_by_length(doc[\"text\"], max_api_length)\n",
    "        for part_idx, text_part in enumerate(text_chunks):\n",
    "            data = {\n",
    "                \"content\": text_part,\n",
    "                \"tokenizer\": \"o200k_base\",\n",
    "                \"return_tokens\": True,\n",
    "                \"return_chunks\": True,\n",
    "                \"max_chunk_length\": 1000\n",
    "            }\n",
    "            response = requests.post(api_url, headers=headers, json=data)\n",
    "            if response.status_code == 200:\n",
    "                print(f\"Successfully processed {doc.get('filename', 'unknown')} part {part_idx+1}/{len(text_chunks)}\")\n",
    "                chunks = response.json().get(\"chunks\", [])\n",
    "                for idx, chunk in enumerate(chunks):\n",
    "                    chunked_data.append({\n",
    "                        \"filename\": doc[\"filename\"],\n",
    "                        \"doc_part\": part_idx,\n",
    "                        \"chunk_index\": idx,\n",
    "                        \"chunk\": chunk\n",
    "                    })\n",
    "            else:\n",
    "                print(f\"Error processing {doc.get('filename', 'unknown')} part {part_idx+1}: {response.status_code} {response.text}\")\n",
    "\n",
    "    # Save chunked data to output file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(chunked_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Example usage\n",
    "chunk_documents(\n",
    "    input_file='extracted.json',\n",
    "    output_file='chunks.json',\n",
    "    api_url='https://api.jina.ai/v1/segment',\n",
    "    api_key=os.getenv('JINA_API_KEY')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d5b7738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total chunks: 873\n",
      "\n",
      "Top 5 longest chunks:\n",
      "\n",
      "Chunk #1 (filename: Perwal Cimahi 6 2024.pdf, part: 0, index: 4, length: 997):\n",
      "\n",
      "Badan adalah sekumpulan orang dan/atau modal yang merupakan kesatuan, baik yang melakukan usaha maupun yang tidak melakukan usaha yang meliputi perseroan terbatas, perseroan komanditer, perseroan lainnya, badan usaha milik negara, badan usaha milik daerah, atau badan usaha milik desa, dengan nama dan dalam bentuk apa pun, firma, kongsi, koperasi, dana pensiun, persekutuan, perkumpulan, yayasan, organisasi masa, organisasi sosial politik, atau organisasi lainnya, lembaga dan bentuk badan lainnya,...\n",
      "\n",
      "Chunk #2 (filename: Perwal Cimahi 6 2024.pdf, part: 1, index: 120, length: 985):\n",
      "\n",
      "n Pajak / Reklame Tahun (Rp/M) Pajak BILLBOARD 1 250,000 375,000 500,000 M2 50,000 1 Tahun / BANDO /3 Bln PAPAN MERK 2 (PMT) 100,000 150,000 200,000 M2 50,000 1 Tahun / MELEKAT/ /3 Bln DINDING DAN BANGUNAN NEON SIGN 3 M2 50,000 1 Tahun / NEON 150,000 200,000 250,000 /3 Bln BOX 4 BALIHO 50,000 75,000 75,000 M2 1 Bulan MEGATRON 5 500,000 750,000 1,000,00 M2 100,000 1 Tahun / 0 /3 Bln VIDIATRON 6 KAIN - 40,000 50,000 50,000 M2 7 Hari SPANDUK 7 POSTER 10,000 12,500 15,000 M2 1 Bulan 8 BERJALAN 7,500...\n",
      "\n",
      "Chunk #3 (filename: Perda Kota Cimahi No. 8 Tahun 2014.pdf, part: 1, index: 111, length: 970):\n",
      "\n",
      "Bagian Ketujuh Pencatatan Perubahan Status Kewarganegaraan Paragraf 1Pencatatan Perubahan Status Kewarganegaraan di Wilayah Negara Kesatuan Republik Indonesia Pasal 101 (1) Perubahan status kewraganegaraan dari warga negara asing menjadi warga negara Indonesia wajib dilaporkan oleh penduduk yang bersangkutan kepada Instansi Pelaksana di tempat peristiwa perubahan status kewarganegaraan paling lambat 60 (enam puluh) hari sejak berita acara pengucapan sumpah atau pernyataan janji setia oleh pejaba...\n",
      "\n",
      "Chunk #4 (filename: Perda Kota Cimahi No. 8 Tahun 2014.pdf, part: 1, index: 140, length: 957):\n",
      "\n",
      "Biodata Penduduk, KK dan KTP-el, Surat Keterangan Pindah Penduduk Warga Negara Republik Indonesia antar Kabupaten/Kota dalam satu Provinsi dan antar Provinsi dalam wilayah Negara Kesatuan Republik Indonesia, Surat Keterangan Pindah Datang Penduduk Warga Negara Indonesia antar Kabupaten/Kota dalam satu Provinsi dan antar Provinsi dalam wilayah Negara Kesatuan Republik Indonesia, Surat Keterangan Pindah Datang Penduduk Orang Asing dalam wilayah Negara Kesatuan Republik Indonesia, Surat Keterangan ...\n",
      "\n",
      "Chunk #5 (filename: Perda Kota Cimahi No. 8 Tahun 2014.pdf, part: 0, index: 88, length: 926):\n",
      "\n",
      "Dalam hal terjadi perubahan elemen data, rusak, atau hilang, Penduduk pemilik KTP-el wajib melaporkan kepada Instansi Pelaksana untuk dilakukan perubahan atau penggantian;(9) Elemen data penduduk tentang agama sebagaimana dimaksud pada ayat (1) bagi Penduduk yang agamanya belum diakui sebagai agama berdasarkan ketentuan Peraturan Perundang-Undangan atau bagi penghayat kepercayaan tidak diisi, tetapi tetap dilayani dan dicatat dalam database kependudukan; (10) Penerbitan KTP-el baru bagi penduduk...\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def show_chunk_stats(chunk_file, n=5):\n",
    "    \"\"\"Display total chunk count and show n longest chunks by length.\"\"\"\n",
    "    with open(chunk_file, 'r', encoding='utf-8') as f:\n",
    "        chunks = json.load(f)\n",
    "    print(f\"Total chunks: {len(chunks)}\\n\")\n",
    "    # Sort by chunk length (descending)\n",
    "    sorted_chunks = sorted(chunks, key=lambda x: len(x['chunk']), reverse=True)\n",
    "    print(f\"Top {n} longest chunks:\")\n",
    "    for i, chunk in enumerate(sorted_chunks[:n]):\n",
    "        print(f\"\\nChunk #{i+1} (filename: {chunk['filename']}, part: {chunk.get('doc_part', 0)}, index: {chunk['chunk_index']}, length: {len(chunk['chunk'])}):\\n\")\n",
    "        print(chunk['chunk'][:500] + ('...' if len(chunk['chunk']) > 500 else ''))\n",
    "\n",
    "# Example usage\n",
    "show_chunk_stats('chunks.json', n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b96336",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f17098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedded batch 1 (1-8) of 873)\n",
      "Embedded batch 2 (9-16) of 873)\n",
      "Embedded batch 3 (17-24) of 873)\n",
      "Embedded batch 4 (25-32) of 873)\n",
      "Embedded batch 5 (33-40) of 873)\n",
      "Embedded batch 6 (41-48) of 873)\n",
      "Embedded batch 7 (49-56) of 873)\n",
      "Embedded batch 8 (57-64) of 873)\n",
      "Embedded batch 9 (65-72) of 873)\n",
      "Embedded batch 10 (73-80) of 873)\n",
      "Embedded batch 11 (81-88) of 873)\n",
      "Embedded batch 12 (89-96) of 873)\n",
      "Embedded batch 13 (97-104) of 873)\n",
      "Embedded batch 14 (105-112) of 873)\n",
      "Embedded batch 15 (113-120) of 873)\n",
      "Embedded batch 16 (121-128) of 873)\n",
      "Embedded batch 17 (129-136) of 873)\n",
      "Embedded batch 18 (137-144) of 873)\n",
      "Embedded batch 19 (145-152) of 873)\n",
      "Embedded batch 20 (153-160) of 873)\n",
      "Embedded batch 21 (161-168) of 873)\n",
      "Embedded batch 22 (169-176) of 873)\n",
      "Embedded batch 23 (177-184) of 873)\n",
      "Embedded batch 24 (185-192) of 873)\n",
      "Embedded batch 25 (193-200) of 873)\n",
      "Embedded batch 26 (201-208) of 873)\n",
      "Embedded batch 27 (209-216) of 873)\n",
      "Embedded batch 28 (217-224) of 873)\n",
      "Embedded batch 29 (225-232) of 873)\n",
      "Embedded batch 30 (233-240) of 873)\n",
      "Embedded batch 31 (241-248) of 873)\n",
      "Embedded batch 32 (249-256) of 873)\n",
      "Embedded batch 33 (257-264) of 873)\n",
      "Embedded batch 34 (265-272) of 873)\n",
      "Embedded batch 35 (273-280) of 873)\n",
      "Embedded batch 36 (281-288) of 873)\n",
      "Embedded batch 37 (289-296) of 873)\n",
      "Embedded batch 38 (297-304) of 873)\n",
      "Embedded batch 39 (305-312) of 873)\n",
      "Embedded batch 40 (313-320) of 873)\n",
      "Embedded batch 41 (321-328) of 873)\n",
      "Embedded batch 42 (329-336) of 873)\n",
      "Embedded batch 43 (337-344) of 873)\n",
      "Embedded batch 44 (345-352) of 873)\n",
      "Embedded batch 45 (353-360) of 873)\n",
      "Embedded batch 46 (361-368) of 873)\n",
      "Embedded batch 47 (369-376) of 873)\n",
      "Embedded batch 48 (377-384) of 873)\n",
      "Embedded batch 49 (385-392) of 873)\n",
      "Embedded batch 50 (393-400) of 873)\n",
      "Embedded batch 51 (401-408) of 873)\n",
      "Embedded batch 52 (409-416) of 873)\n",
      "Embedded batch 53 (417-424) of 873)\n",
      "Embedded batch 54 (425-432) of 873)\n",
      "Embedded batch 55 (433-440) of 873)\n",
      "Embedded batch 56 (441-448) of 873)\n",
      "Embedded batch 57 (449-456) of 873)\n",
      "Embedded batch 58 (457-464) of 873)\n",
      "Embedded batch 59 (465-472) of 873)\n",
      "Embedded batch 60 (473-480) of 873)\n",
      "Embedded batch 61 (481-488) of 873)\n",
      "Embedded batch 62 (489-496) of 873)\n",
      "Embedded batch 63 (497-504) of 873)\n",
      "Embedded batch 64 (505-512) of 873)\n",
      "Embedded batch 65 (513-520) of 873)\n",
      "Embedded batch 66 (521-528) of 873)\n",
      "Embedded batch 67 (529-536) of 873)\n",
      "Embedded batch 68 (537-544) of 873)\n",
      "Embedded batch 69 (545-552) of 873)\n",
      "Embedded batch 70 (553-560) of 873)\n",
      "Embedded batch 71 (561-568) of 873)\n",
      "Embedded batch 72 (569-576) of 873)\n",
      "Embedded batch 73 (577-584) of 873)\n",
      "Embedded batch 74 (585-592) of 873)\n",
      "Embedded batch 75 (593-600) of 873)\n",
      "Embedded batch 76 (601-608) of 873)\n",
      "Embedded batch 77 (609-616) of 873)\n",
      "Embedded batch 78 (617-624) of 873)\n",
      "Embedded batch 79 (625-632) of 873)\n",
      "Embedded batch 80 (633-640) of 873)\n",
      "Embedded batch 81 (641-648) of 873)\n",
      "Embedded batch 82 (649-656) of 873)\n",
      "Embedded batch 83 (657-664) of 873)\n",
      "Embedded batch 84 (665-672) of 873)\n",
      "Embedded batch 85 (673-680) of 873)\n",
      "Embedded batch 86 (681-688) of 873)\n",
      "Embedded batch 87 (689-696) of 873)\n",
      "Embedded batch 88 (697-704) of 873)\n",
      "Embedded batch 89 (705-712) of 873)\n",
      "Embedded batch 90 (713-720) of 873)\n",
      "Embedded batch 91 (721-728) of 873)\n",
      "Embedded batch 92 (729-736) of 873)\n",
      "Embedded batch 93 (737-744) of 873)\n",
      "Embedded batch 94 (745-752) of 873)\n",
      "Embedded batch 95 (753-760) of 873)\n",
      "Embedded batch 96 (761-768) of 873)\n",
      "Embedded batch 97 (769-776) of 873)\n",
      "Embedded batch 98 (777-784) of 873)\n",
      "Embedded batch 99 (785-792) of 873)\n",
      "Embedded batch 100 (793-800) of 873)\n",
      "Embedded batch 101 (801-808) of 873)\n",
      "Embedded batch 102 (809-816) of 873)\n",
      "Embedded batch 103 (817-824) of 873)\n",
      "Embedded batch 104 (825-832) of 873)\n",
      "Embedded batch 105 (833-840) of 873)\n",
      "Embedded batch 106 (841-848) of 873)\n",
      "Embedded batch 107 (849-856) of 873)\n",
      "Embedded batch 108 (857-864) of 873)\n",
      "Embedded batch 109 (865-872) of 873)\n",
      "Embedded batch 110 (873-873) of 873)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "def embed_chunks_batch(chunk_file, output_file, api_url, api_key, model=\"jina-embeddings-v3\", batch_size=8, sleep_time=0.5):\n",
    "    \"\"\"\n",
    "    Embed all chunks using Jina Embeddings API in batches and save the results to a JSON file.\n",
    "    \"\"\"\n",
    "    with open(chunk_file, 'r', encoding='utf-8') as f:\n",
    "        chunks = json.load(f)\n",
    "\n",
    "    embeddings = []\n",
    "    headers = {\n",
    "        'Content-Type': 'application/json',\n",
    "        'Authorization': f'Bearer {api_key}'\n",
    "    }\n",
    "\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i:i+batch_size]\n",
    "        texts = [c[\"chunk\"] for c in batch]\n",
    "        data = {\n",
    "            \"model\": model,\n",
    "            \"task\": \"retrieval.passage\",\n",
    "            \"late_chunking\": True,\n",
    "            \"input\": texts\n",
    "        }\n",
    "        response = requests.post(api_url, headers=headers, json=data)\n",
    "        if response.status_code == 200:\n",
    "            emb_list = response.json().get(\"data\", [])\n",
    "            for c, emb in zip(batch, emb_list):\n",
    "                embeddings.append({\n",
    "                    \"filename\": c[\"filename\"],\n",
    "                    \"doc_part\": c.get(\"doc_part\", 0),\n",
    "                    \"chunk_index\": c[\"chunk_index\"],\n",
    "                    \"embedding\": emb.get(\"embedding\", [])\n",
    "                })\n",
    "            print(f\"Embedded batch {i//batch_size+1} ({i+1}-{i+len(batch)}) of {len(chunks)})\")\n",
    "        else:\n",
    "            print(f\"Error embedding batch {i//batch_size+1}: {response.status_code} {response.text}\")\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(embeddings, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Example usage\n",
    "embed_chunks_batch(\n",
    "    chunk_file='chunks.json',\n",
    "    output_file='embeddings.json',\n",
    "    api_url='https://api.jina.ai/v1/embeddings',\n",
    "    api_key=os.getenv('JINA_API_KEY'),\n",
    "    batch_size=8\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "844aaf2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS index created with 873 vectors and saved to 'faiss_index'.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Load embeddings from embeddings.json\n",
    "with open('embeddings.json', 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract vectors and (optionally) metadata\n",
    "vectors = [item['embedding'] for item in data]\n",
    "# Optionally, save metadata for later retrieval\n",
    "metadata = [\n",
    "    {\n",
    "        'filename': item.get('filename'),\n",
    "        'doc_part': item.get('doc_part'),\n",
    "        'chunk_index': item.get('chunk_index')\n",
    "    }\n",
    "    for item in data\n",
    "]\n",
    "\n",
    "# Convert to numpy array (float32)\n",
    "vecs_np = np.array(vectors, dtype='float32')\n",
    "\n",
    "# Create FAISS index (L2 similarity)\n",
    "dim = vecs_np.shape[1]\n",
    "index = faiss.IndexFlatL2(dim)\n",
    "index.add(vecs_np)\n",
    "\n",
    "# Save index to file\n",
    "faiss.write_index(index, 'faiss_index')\n",
    "\n",
    "# Optionally, save metadata for later use\n",
    "with open('faiss_metadata.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"FAISS index created with {index.ntotal} vectors and saved to 'faiss_index'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
